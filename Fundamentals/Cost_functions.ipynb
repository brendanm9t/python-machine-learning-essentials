{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Quick introduction to Cost Functions</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions versus Cost functions\n",
    "* A Loss is calculated for a single data entry. A function used to calculate the loss is called the **loss function**\n",
    "* Cost is the calculated loss for the entire dataset. A function used to calculate the cost is called the **cost function**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Absolute Error (MAE)\n",
    "\n",
    "Mean Absoluter Error (**MAE**) is the **Absolute** difference between the training and the predicted data. \n",
    "* **MAE** is robust against outliers\n",
    "* Low penalisation factor against outliers\n",
    "* **MAE** is not differentiable at zero\n",
    "\n",
    "The residual of a single data entry - **loss function**:\n",
    "\n",
    "***Mean Absolute Loss*** = $| y - \\hat{y}| $\n",
    "\n",
    "The residuals of the entire dataset - **cost function**:\n",
    "\n",
    "***Mean Absolute Error*** =$ \\frac{1}{n} \\sum\\limits_{i=1}^{n}| y_i - \\hat{y_i}| $\n",
    "\n",
    "$y$ = *Actual value*\n",
    "\n",
    "$\\hat{y} = b + w_1x_1 + w_2x_2 + .... + w_nx_n$ **(in case of regression)**\n",
    "\n",
    "* $b $ = *Bias*\n",
    "\n",
    "* $w_i $ = *Weights/ coefficients*\n",
    "\n",
    "* $x_i$ = *Features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "def mean_absolute_error(y_train, y_prediction):\n",
    "    \"\"\"\n",
    "    Returns Mean Absolute Error \n",
    "    \"\"\"\n",
    "    abs_error = abs(y_train - y_prediction)\n",
    "    mean_abs_error = abs_error.mean()\n",
    "    return mean_abs_error\n",
    "\n",
    "mean_absolute_error(y_train, y_prediction)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Squared Error (MSE)\n",
    "Mean Squared Error (**MSE**) is the squared difference between the training and the predicted data. \n",
    "* **MSE** is very sensitive to outliers\n",
    "* High penalisation factor against outliers\n",
    "* **MSE** is used when high errors are undesirable\n",
    "\n",
    "The residual of a single data entry - **loss function**:\n",
    "\n",
    "***Mean Squared Loss*** = $(y - \\hat{y})^2$\n",
    "\n",
    "The residuals of the entire dataset - **cost function**:\n",
    "\n",
    "***Mean Squared Error*** =$ \\frac{1}{n}\\sum\\limits_{i=1}^{n} (y_i - \\hat{y_i})^2 $\n",
    "\n",
    "$y$ = *Actual value*\n",
    "\n",
    "$\\hat{y} = b + w_1x_1 + w_2x_2 + .... + w_nx_n$ \n",
    "\n",
    "* $b $ = *Bias*\n",
    "\n",
    "* $w_i $ = *Weights/ coefficients*\n",
    "\n",
    "* $x_i$ = *Features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "def mean_squared_error(y_train, y_prediction):\n",
    "    \"\"\"\n",
    "    Returns Mean Squared Error \n",
    "    \"\"\"\n",
    "    square_error = (y_train - predictions) ** 2\n",
    "    mean_square_error = square_error.mean()\n",
    "    return mean_square_error\n",
    "\n",
    "mean_squared_error(y_train, y_prediction)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root Mean Squared Error (RMSE)\n",
    "Root Mean Squared Error (**RMSE**)is the squared mean of the difference between the training and the predicted data. \n",
    "* **RMSE** is highly sensitive to outliers\n",
    "* High penalisation factor against outliers, but not as much as **MSE**\n",
    "\n",
    "The residual of a single data entry - **loss function**:\n",
    "\n",
    "***Root Mean Squared Loss*** = $\\sqrt{ (y_i - \\hat{y_i})^2}$\n",
    "\n",
    "The residuals of the entire dataset - **cost function**:\n",
    "\n",
    "***Root Mean Squared Error*** =$  \\sqrt{\\frac{1}{n} \\sum\\limits_{i =1}^{n}(y_i - \\hat{y_i})^2}$\n",
    "\n",
    "$y$ = *Actual value*\n",
    "\n",
    "$\\hat{y} = b + w_1x_1 + w_2x_2 + .... + w_nx_n$ \n",
    "\n",
    "* $b $ = *Bias*\n",
    "\n",
    "* $w_i $ = *Weights/ coefficients*\n",
    "\n",
    "* $x_i$ = *Features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "def root_mean_squared_error(y_train, y_prediction):\n",
    "    \"\"\"\n",
    "    Returns Root Mean Squared Error\n",
    "    \"\"\"\n",
    "    square_error = (y_train - predictions) ** 2\n",
    "    mean_square_error = square_error.mean()\n",
    "    rmse = np.sqrt(mean_square_error)\n",
    "    return root_mean_squared_error\n",
    "\n",
    "root_mean_squared_error(y_train, y_prediction)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root Mean Squared Logarithmic Error (RMSLE)\n",
    "Root Mean Squared Logarithmic Error (**RMSLE**) is the squared mean of the logarithmic difference between the training and the predicted data.\n",
    "* **RMSLE** is used when the target is not normalised/scaled.\n",
    "* **RMSLE** is less sensitive to outliers than **RMSE**\n",
    "* Medium penalisation factor against outliers\n",
    "\n",
    "The residual of a single data entry - **loss function**:\n",
    "\n",
    "***Root Mean Squared Logarithmic Loss*** = $  \\sqrt{(log(y_i + 1) - log(\\hat{y_i}+1))^2}$\n",
    "\n",
    "The residuals of the entire dataset - **cost function**:\n",
    "\n",
    "***Root Mean Squared Logarithmic Error*** =$  \\sqrt{\\frac{1}{n} \\sum\\limits_{i =1}^{n}(log(y_i + 1) - log(\\hat{y_i}+1))^2}$\n",
    "\n",
    "$y$ = *Actual value*\n",
    "\n",
    "$\\hat{y} = b + w_1x_1 + w_2x_2 + .... + w_nx_n$ \n",
    "\n",
    "* $b $ = *Bias*\n",
    "\n",
    "* $w_i $ = *Weights/ coefficients*\n",
    "\n",
    "* $x_i$ = *Features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "def root_mean_squared_log_error(y_train, y_prediction):\n",
    "    \"\"\"\n",
    "    Returns Root Mean Square Log Error\n",
    "    \"\"\"\n",
    "    square_error = (np.log(y_train+1) - np.log(predictions+1)) ** 2\n",
    "    mean_square_log_error = square_error.mean()\n",
    "    rmsle = np.sqrt(mean_square_log_error)\n",
    "    return root_mean_squared_log_error\n",
    "\n",
    "root_mean_squared_log_error(y_train, y_prediction)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "__<a href=\"https://towardsdatascience.com/cost-functions-of-regression-and-its-optimization-techniques-in-machine-learning-2f5931cd33f1\">Link to Reference</a>__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
